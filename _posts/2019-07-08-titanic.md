---
title: "Machine Learning Project: Titanic"
date: 2019-07-08
tags: [machine learning, data science]
header:
  image: "/images/titaniccover.jpg"

  excerpt: "Machine Learning, Data Science"

---

# Survival Prediction Using Machine Learning

In this post, I will go over how I analyze and build a model using Titanic data set. This was the very first data science project that I built a predictive model when I just started data science journey.

## Data
The data is from a Kaggle competition called [*Titanic: Machine Learning from Disaster*](https://www.kaggle.com/c/titanic/overview).
Dataset consists of train and test data, of which both contain information about Titanic passengers, including ticket class, sex, age, cabin number, etc. For train data, it also has a field called "survival", representing if the passenger survived or not. Train data has 891 rows, while test data has 418 rows.

## Model
After preping data, a series of different machine learning algorithms are considered.
One important note for data modeling is that there is no super algorithm that can work best for all types of data, as stated by the **No Free Lunch Theorem**. It essentially says that each scenario requires different algorithms and parameters (or hyperparameters) to achieve the best result.

With that being said, however, there are a few options that one can start, namely Trees, Bagging, Random Forests and Boosting. They all come from the concept of a decision tree, which is quite easy to understand, comparing to other "black box", such as SVM.

There are total of 22 algorithms that I will use in this project, which can be classified into a few areas, including ensemble methods, Gaussian, General Linear Model (GLM), Naive Bayes, Nearest Neighbor, SVM, Trees, Discriminant Analysis, and XGBoost.

**Important:** Often times we don't need to build that many models. In my experience, we should narrow down the list to about 5 models, and try to validate from there.

## Model Validation

Accuracy score is recorded for each model. First, they will be compared against base line accuracy, which, FYI, is not 50% in this case, even though it is a binary classification. Since we have data on passengers, we could somehow make a more educated guess than just pure luck. Using just some logic to build a simple decision tree, we could see that the base model is about 82% accurate. I will discuss this problem in another time to keep this post short and succinct.

Here is the result after those models are being tuned and optimized.

<img src="{{ site.url }}{{ site.baseurl }}/images/titanic/modelscompare.jpg" alt="">

## Conclusion
In this example, XGBoost seems to perform best. However, other algorithms follow really close, including Random Forest, SVM, and Gradient Boosting.

### Code
Visit my [GitHub repo](https://github.com/shoang5011/Titanic) for reference.
