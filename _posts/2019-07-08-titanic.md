---
title: "Machine Learning Project: Titanic"
date: 2019-07-08
tags: [machine learning, data science]
header:
  image: "/images/titaniccover.jpg"

  excerpt: "Machine Learning, Data Science"

---

# Survival Prediction Using Machine Learning

In this post, I will go over how I analyze and build a predictive model using Titanic data set. This was the very first data science project when I just started learning about this field, which was 2 years ago.
* Note: I still working on organizing this post to be more detailed and will be adding some modificaitons.  

## Data
The data is from a Kaggle competition called [*Titanic: Machine Learning from Disaster*](https://www.kaggle.com/c/titanic/overview).
Dataset consists of train and test data, of which both contain information about Titanic passengers, including ticket class, sex, age, cabin number, etc. For train data, it also has a field called "survival", representing if the passenger survived or not. Train data has 891 rows, while test data has 418 rows.

##EDA
In this step, data a first explored through several methods; one of which is visualization, of which I find the most useful when it comes to EDA. This is an example of visualization on missing data:
<img src="{{ site.url }}{{ site.baseurl }}/images/titanic/missingdata.jpg" alt="">

The relationship between fields and label can also be demonstrated through graphs.
<img src="{{ site.url }}{{ site.baseurl }}/images/titanic/embark.jpg" alt="">
<img src="{{ site.url }}{{ site.baseurl }}/images/titanic/age.jpg" alt="">

## Model
After preping data, a series of different machine learning algorithms are considered.
One important note for data modeling is that there is no super algorithm that can work best for all types of data, as stated by the **No Free Lunch Theorem**, which essentially says that different scenario requires different algorithms and parameters (or hyperparameters) to achieve the best result.

With that being said, however, there are a few options that one can start, namely Trees, Bagging, Random Forests and Boosting. They all come from the concept of a decision tree, which is quite easy to understand, comparing to other "black box", such as SVM.

There are total of 8 algorithms that I will use in this project, which can be classified into a few areas, including ensemble methods, Gaussian, General Linear Model (GLM), Naive Bayes, Nearest Neighbor, SVM, Trees, and XGBoost.

**Important:** Often times we don't need to build too many models. In my experience, between 5 to 8 models is a sweet spot. Then, we can fine tune from there.

## Model Validation

Accuracy score is recorded for each model. First, they will be compared against base line accuracy, which, FYI, is not 50% in this case, even though it is a binary classification. Since we have data on passengers, we could somehow make a more educated guess than just pure luck. Using just some logic to build a simple decision tree, we could see that the base model is about 82% accurate. I will discuss this problem in another time to keep this post short and to the point.

Here is the result after those models are being tuned and optimized.

<img src="{{ site.url }}{{ site.baseurl }}/images/titanic/MLAscore.jpg" alt="">

## Conclusion
In this example, XGBoost seems to perform best. Other algorithms follow really close, including Random Forest, SVM, and Gradient Boosting.
Strangely enough, however, these models achieve lower score than that of base line model, which is a simple decision tree that I mentioned. One can say that because this data set is quite small, so complicated models that are often used on larger set (Gradient Boosting, for example) do not provide an edge over simplified models.
