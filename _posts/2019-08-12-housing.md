---
title: "Machine Learning Project: Housing Price Prediction - Top 4%"
date: 2019-08-12
tags: [machine learning, data science]
header:
  image: "/images/housing/cover.jpeg"

  excerpt: "Machine Learning, Data Science"

---

# House Price Prediction Using Machine Learning

This is one of the projects I competed on Kaggle against 4,300 teams, and ended up being in top 4% on leader board. By taking part in this, I learned how to use ensemble learning, which is, to combine different machine learning models to increase predictive power of the final model.

## Data
The data is from a Kaggle competition called [*House Prices: Advance Regression Techniques*](https://www.kaggle.com/c/house-prices-advanced-regression-techniques/overview).
As usual, data consists of train and test data, where train data is used to train models, and then predict using test data for submission. Data has 80 features and about 2,000 samples.

## EDA
Train data is visualized by several methods for easier analysis. Visualization is one of the most effective way to explore and understand a data set. Therefore, in this post, I will also cover different types of graphs that can be used in different cases.
First, I will analyze dependent variable, which is SalePrice, using histogram plot to show distribution.
<img src="{{ site.url }}{{ site.baseurl }}/images/housing/price.png" alt="">

Then, I use scatter plot and box plot to analyze other independent variables.
<img src="{{ site.url }}{{ site.baseurl }}/images/housing/livingarea.png" alt="">
<img src="{{ site.url }}{{ site.baseurl }}/images/housing/overallqual.png" alt="">

* Note: For more visualization techniques for bivariate and multi-variate analysis, I recommend checking out Titanic project for further explanation and code snippet.

Missing data is a very serious problem in this project, since some of the features lack almost 100% info.
<img src="{{ site.url }}{{ site.baseurl }}/images/housing/missing.jpg" alt="">

For this specific project, missing values can be replaced by either None or 0 (it is not technically "missing values", as it is not available due to being 0 or does not have any). Some features require imputation since no information is provided.

EDA is the very first step to get to know of data, and decide what analyses are necessary for later data engineering and  model building.

## Data Engineering
In this step, some new features that are created from available features. For example, I create TotalSF column, which is the sum of TotalBsmtSF (total basement square footage), 1stFlrSF (first floor square footage) and 2ndFlSF (second floor square footage). These 3 latter columns are provided in the data set. By creating TotalSF column, the final model will have more power in prediction.
The same goes to other columns where we could create more useful and informative features for learning algorithm. For example,

## Outliers
To deal with outliers, I usually choose to build a quick model, and determine if a point is outlier or not by calculating a score that follow the equation:
$$z = (residual - mean_residual) / std$$
For this project, I build a model using Ridge, and then remove outliers based on calculated score. This method eventually improves the final score.
Outliers are shown in the graph below.
<img src="{{ site.url }}{{ site.baseurl }}/images/housing/livingarea.png" alt="">

## Modeling
After outliers being removed, some machine learning algorithms are considered for modeling. For this problem, it would make sense to use gradient boosting algorithm and Lasso.
Hyper-parameters tuning is required. To do that, I use GridSearchCV package to optimize the input parameters.
Finally, after having constructed multiple models using different algorithms, I use a stacking technique which I learned from previous competitions; that is, by combining different models with different weights, I can come up with one unified model that could perform better than each previous individual model. This is one of the secret sauces to win a Kaggle competition.  

## Summary
I have built a predictive model utilizing several machine learning algorithms. With ensemble technique mentioned, I am able to improve my score to about 5%, which might not seem very significant, but it actually helped me climbing up 30 more ranks.
This project deepened my machine learning knowledge significantly, and it is a very good practice where I can apply new concepts and complicate models.
Of course there is still room for improvement. There are several things on top of my head I can further do such as more extensive feature engineering, removing noisy features (this requires detail analysis for each feature). Even though they can potentially improve model's score, I decided not to carry on with those as I think it is not really important in practice. Spending much more effort to just increase a few points in score is unreasonable in this case. My goal entering this competition was not to win but to learn new important techniques (ensemble learning and gradient boosting algorithm), so I am quite happy with it now.  

### Code
Visit my [*GitHub repo*](https://github.com/shoang5011/HousingPrice-Kaggle) to see the code.
