---
title: "Machine Learning Project: Housing Price Prediction - Top 4%"
date: 2019-08-12
tags: [machine learning, data science]
header:
  image: "/images/housing/cover3.jpg"

  excerpt: "Machine Learning, Data Science"

---

# House Price Prediction Using Machine Learning

This is one of the projects I competed on Kaggle against 4,300 teams, and ended up being in top 4% on leaderboard. By taking part in this, I learned how to use ensemble learning, which is, to combine different machine learning models to increase predictive power of the final model.

## Data
The data is from a Kaggle competition called [*House Prices: Advance Regression Techniques*](https://www.kaggle.com/c/house-prices-advanced-regression-techniques/overview).

As usual, data consists of train and test data, where train data is used to train models, and then predict using test data for submission. Data has 80 features and about 2,000 samples.

## EDA
Train data is visualized by several methods for easier analysis. Visualization is one of the most effective way to explore and understand a data set. Therefore, in this post, I will also cover different types of graphs that can be used in different cases.

First, I will analyze dependent variable, which is SalePrice, using histogram plot to show distribution.
<img src="{{ site.url }}{{ site.baseurl }}/images/housing/price.png" alt="">

Then, I use scatter plot and box plot to analyze other independent variables.
<img src="{{ site.url }}{{ site.baseurl }}/images/housing/livingarea.png" alt="">
<img src="{{ site.url }}{{ site.baseurl }}/images/housing/overallqual.png" alt="">

* Note: For more visualization techniques for bivariate and multi-variate analysis, I recommend checking out Titanic project for further explanation and code snippet.

Missing data is a very serious problem in this project, since some of the features lack almost 100% info.

<img src="{{ site.url }}{{ site.baseurl }}/images/housing/missing.png" alt="">

For this specific project, missing values can be replaced by either None or 0 (it is not technically "missing values", as it is not available due to being 0 or does not have any). Some features require imputation since no information is provided.

EDA is the very first step to get to know of data, and decide what analyses are necessary for later data engineering and  model building.

## Data Engineering
In this step, some new features that are created from available features. For example, I create TotalSF column, which is the sum of TotalBsmtSF (total basement square footage), 1stFlrSF (first floor square footage) and 2ndFlSF (second floor square footage). These 3 latter columns are provided in the data set. By creating TotalSF column, the final model will have more power in prediction. The same goes to other columns where we could create more useful and informative features for learning algorithm.

## Outliers
To deal with outliers, I use cut off value being 3 times standard deviation.

For this project, I build a model using Ridge for quick demonstration, and then remove outliers using mentioned cut off value. This method eventually improves the final score.
Outliers are shown in the graph below.

<img src="{{ site.url }}{{ site.baseurl }}/images/housing/outliers.png" alt="">

## Modeling
After outliers being removed, some machine learning algorithms are considered for modeling. For this problem, it would make sense to use gradient boosting algorithm and Lasso.

Hyper-parameters tuning is required. To do that, I use GridSearchCV package to optimize the input parameters.
Finally, after having constructed multiple models using different algorithms, I use a stacking technique which I learned from previous competitions; that is, by combining different models with different weights, I can come up with one unified model that could perform better than each previous individual model. This is one of the secret sauces to win a Kaggle competition.  


## Summary
I have built a predictive model utilizing several machine learning algorithms. With ensemble technique mentioned, I am able to improve my score, and climb 30 more ranks on leaderboard, and ranked within top 4% overall.

<img src="{{ site.url }}{{ site.baseurl }}/images/housing/leaderboard.png" alt="">

This project deepened my machine learning knowledge significantly, and it is a very good practice where I can apply new concepts and complicate models.

Of course there is still room for improvement. There are several things on top of my head I can further do such as more feature engineering, removing noisy features (this requires detail analysis for each feature). This, however, has fulfilled my goal up to this point, which is to apply new techniques that I have learned (ensemble learning and gradient boosting algorithm), so I decide not to carry on. This is also a problem in practice, since we have to decide if increasing a few points in score would be worth the extra effort.  

### Code
Visit my [*GitHub repo*](https://github.com/shoang5011/HousingPrice-Kaggle) to see the code.
